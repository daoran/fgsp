{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pygsp import graphs, filters, plotting, reduction\n",
    "import pandas\n",
    "from enum import Enum\n",
    "import scipy.spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import os.path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plotting.BACKEND = 'matplotlib'\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmapState(Enum):\n",
    "    ALL_GOOD = 1\n",
    "    GLOBAL_BAD = 2\n",
    "    MID_BAD = 3\n",
    "    LOCAL_BAD = 4\n",
    "    NO_GOOD = 5\n",
    "\n",
    "class WaveletEvaluator(object):\n",
    "    def __init__(self, n_scales = 7):\n",
    "        self.n_scales = n_scales\n",
    "\n",
    "    def set_scales(self, n_scales):\n",
    "        self.n_scales = n_scales\n",
    "\n",
    "    def compare_signals(self, G, x_1, x_2):\n",
    "\n",
    "        # Compute the wavelets for each node and scale.\n",
    "        psi = self.compute_wavelets(G)\n",
    "        print(f\"[WaveletEvaluator] psi = {psi.shape}\")\n",
    "\n",
    "        # Compute the wavelet coefficients for x_1 and x_2.\n",
    "        W_1 = self.compute_wavelet_coefficients(psi, x_1)\n",
    "        W_2 = self.compute_wavelet_coefficients(psi, x_2)\n",
    "        print(f\"[WaveletEvaluator] W_1 = {W_1.shape}\")\n",
    "        print(f\"[WaveletEvaluator] W_2 = {W_2.shape}\")\n",
    "\n",
    "\n",
    "    def compute_wavelets(self, G):\n",
    "        print(f\"[WaveletEvaluator] Computing wavelets for {self.n_scales} scales.\")\n",
    "        g = filters.Meyer(G, self.n_scales)\n",
    "\n",
    "        # Evalute filter bank on the frequencies (eigenvalues).\n",
    "        f = g.evaluate(G.e)\n",
    "        f = np.expand_dims(f.T, 1)\n",
    "        psi = np.zeros((G.N, G.N, self.n_scales))\n",
    "\n",
    "        for i in range(0, G.N):\n",
    "\n",
    "            # Create a Dirac centered at node i.\n",
    "            x = np.zeros((G.N,1))\n",
    "            x[i] = 1\n",
    "\n",
    "            # Transform the signal to spectral domain.\n",
    "            s = G._check_signal(x)\n",
    "            s = G.gft(s)\n",
    "\n",
    "            # Multiply the transformed signal with filter.\n",
    "            if s.ndim == 1:\n",
    "                s = np.expand_dims(s, -1)\n",
    "            s = np.expand_dims(s, 1)\n",
    "            s = np.matmul(s, f)\n",
    "\n",
    "            # Transform back the features to the vertex domain.\n",
    "            psi[i, :, :] = G.igft(s).squeeze()\n",
    "\n",
    "        return psi\n",
    "\n",
    "    def compute_wavelets_coeffs(self, wavelet, x_signal):\n",
    "        n_values = x_signal.shape[0]\n",
    "        W = np.zeros((n_values, self.n_scales))\n",
    "        for i in range(0, n_values):\n",
    "            for j in range(0, self.n_scales):\n",
    "                W[i,j] = np.matmul(wavelet[i,:,j].transpose(), x_signal)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def check_submap(self, coeffs_1, coeffs_2, submap_ids):\n",
    "        submap_coeffs_1 = coeffs_1[submap_ids, :]\n",
    "        submap_coeffs_2 = coeffs_2[submap_ids, :]\n",
    "\n",
    "        D = self.compute_generic_distance(submap_coeffs_1, submap_coeffs_2)\n",
    "        return self.evaluate_scales(D)\n",
    "    \n",
    "    def compute_features_for_submap(self, coeffs_1, coeffs_2, submap_ids):\n",
    "        submap_coeffs_1 = coeffs_1[submap_ids, :]\n",
    "        submap_coeffs_2 = coeffs_2[submap_ids, :]\n",
    "        \n",
    "        D = self.compute_distances(submap_coeffs_1, submap_coeffs_2)\n",
    "                              \n",
    "        data = np.array([            \n",
    "            # Euclidean distance.\n",
    "            np.sum(D[0, 0:2]),\n",
    "            np.sum(D[0, 2:4]),\n",
    "            np.sum(D[0, 5:]),            \n",
    "            \n",
    "            # Correlation.\n",
    "            np.sum(D[1, 0:2]),\n",
    "            np.sum(D[1, 2:4]),\n",
    "            np.sum(D[1, 5:]),                        \n",
    "            \n",
    "            # Cityblock distance.\n",
    "            np.sum(D[2, 0:2]),\n",
    "            np.sum(D[2, 2:4]),\n",
    "            np.sum(D[2, 5:]),\n",
    "            \n",
    "            # Chebyshev distance.\n",
    "            np.sum(D[3, 0:2]),\n",
    "            np.sum(D[3, 2:4]),\n",
    "            np.sum(D[3, 5:]),            \n",
    "        ])\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def compute_distances(self, coeffs_1, coeffs_2):\n",
    "        distances = np.zeros((9, self.n_scales))            \n",
    "        for j in range(0, self.n_scales):                              \n",
    "            distances[0, j] = scipy.spatial.distance.euclidean(coeffs_1[:,j], coeffs_2[:,j])            \n",
    "            distances[1, j] = scipy.spatial.distance.correlation(coeffs_1[:,j], coeffs_2[:,j])\n",
    "            distances[2, j] = scipy.spatial.distance.cityblock(coeffs_1[:,j], coeffs_2[:,j])         \n",
    "            distances[3, j] = scipy.spatial.distance.chebyshev(coeffs_1[:,j], coeffs_2[:,j])\n",
    "            \n",
    "        return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /mnt/data/datasets/fgsp/gonzen_mission_03_cerberus/\n",
      "Loading data from /mnt/data/datasets/fgsp/lee_h_mission_01_cerberus/\n",
      "=======================================\n",
      "Finished loading 2 datasets and 2 trajectories.\n"
     ]
    }
   ],
   "source": [
    "dataroot = '/mnt/data/datasets/fgsp/'\n",
    "datasets = ['gonzen_mission_03_cerberus/', 'lee_h_mission_01_cerberus/']\n",
    "\n",
    "n_datasets = len(datasets)\n",
    "opt_signals = []\n",
    "est_signals = []\n",
    "opt_trajectory = []\n",
    "est_trajectory = []\n",
    "opt_graph_coords = []\n",
    "opt_graph_adj = []\n",
    "for ds in datasets:\n",
    "    ds_path = dataroot + ds\n",
    "    print(f'Loading data from {ds_path}')\n",
    "    opt_signal_path = ds_path + 'opt_signal.npy'\n",
    "    opt_traj_path = ds_path + 'opt_trajectory.npy'\n",
    "    est_signal_path = ds_path + 'est_signal.npy'\n",
    "    est_traj_path = ds_path + 'est_trajectory.npy'  \n",
    "    opt_graph_coords_path = ds_path + 'opt_graph_coords.npy'\n",
    "    opt_graph_adj_path = ds_path + 'opt_graph_adj.npy'\n",
    " \n",
    "    if os.path.isfile(opt_signal_path) and os.path.isfile(est_signal_path):\n",
    "        opt_signals.append(np.load(opt_signal_path))\n",
    "        est_signals.append(np.load(est_signal_path))\n",
    "        \n",
    "    if os.path.isfile(opt_traj_path) and os.path.isfile(est_traj_path):\n",
    "        opt_trajectory.append(np.load(opt_traj_path))\n",
    "        est_trajectory.append(np.load(est_traj_path))\n",
    "    \n",
    "    if os.path.isfile(opt_graph_coords_path) and os.path.isfile(opt_graph_adj_path):\n",
    "        opt_graph_coords.append(np.load(opt_graph_coords_path))\n",
    "        opt_graph_adj.append(np.load(opt_graph_adj_path))\n",
    "    \n",
    "print(f'=======================================')\n",
    "n_datasets = len(opt_signals)\n",
    "n_trajectories = len(opt_trajectory)\n",
    "assert n_datasets == len(est_signals)\n",
    "assert n_trajectories == len(opt_trajectory)\n",
    "print(f'Finished loading {n_datasets} datasets and {n_trajectories} trajectories.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "## Learn a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pandas.DataFrame([])\n",
    "feature_names = ['Euclidean_L',\n",
    "                 'Euclidean_B',\n",
    "                 'Euclidean_H',\n",
    "                 'Correlation_L',\n",
    "                 'Correlation_B',\n",
    "                 'Correlation_H',                 \n",
    "                 'Manhattan_L',\n",
    "                 'Manhattan_B',\n",
    "                 'Manhattan_H',\n",
    "                 'Chebyshev_L',\n",
    "                 'Chebyshev_B',\n",
    "                 'Chebyshev_H']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-based Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_labels_for_signals(est_signal, opt_signal, submap_indices):    \n",
    "    n_est_signal = len(est_signal)\n",
    "    n_opt_signal = len(opt_signal)        \n",
    "    assert n_est_signal == n_opt_signal\n",
    "    for i in submap_indices:\n",
    "        diff = np.abs(est_signal[i] - opt_signal[i])\n",
    "        if diff > 0.9:\n",
    "            return 1\n",
    "        \n",
    "    return 0          \n",
    "\n",
    "labels = {}\n",
    "for ds_idx in range(0, n_datasets):\n",
    "    print(f'Computing data for {datasets[ds_idx]}')\n",
    "    # Compute wavelets.\n",
    "    eval = WaveletEvaluator()\n",
    "    psi = eval.compute_wavelets(graph_per_dataset[ds_idx])\n",
    "    print(f\" psi = {psi.shape}\")\n",
    "    \n",
    "    W_1 = eval.compute_wavelets_coeffs(psi, est_signals[ds_idx])\n",
    "    W_2 = eval.compute_wavelets_coeffs(psi, opt_signals[ds_idx])\n",
    "    \n",
    "    n_submaps = 40\n",
    "    n_nodes_in_graph = graph_per_dataset[ds_idx].N\n",
    "#     submap_size = int(n_nodes_in_graph/n_submaps)\n",
    "    submap_size = 3\n",
    "    \n",
    "    indices = []\n",
    "    submap_indices = []\n",
    "    state_signal = np.zeros((n_nodes_in_graph, 1))\n",
    "    submap_data = []\n",
    "    submap_labels = []\n",
    "    \n",
    "    for i in range(0, n_nodes_in_graph):\n",
    "        n_indices_in_submap = len(submap_indices)\n",
    "\n",
    "        # If the submap reached max size, evaluate it.\n",
    "        if n_indices_in_submap > 0 and n_indices_in_submap % submap_size == 0:        \n",
    "            features = eval.compute_features_for_submap(W_1, W_2, submap_indices)\n",
    "            label = compute_labels_for_signals(est_signals[ds_idx], opt_signals[ds_idx], submap_indices)\n",
    "            \n",
    "            submap_data.append(features)\n",
    "            submap_labels.append(label)\n",
    "            submap_indices = []\n",
    "\n",
    "        submap_indices.append(i)\n",
    "    \n",
    "    submap_data = np.array(submap_data)\n",
    "    submap_labels = np.array(submap_labels)    \n",
    "    print(f'labels {submap_labels.shape} and submap_data {submap_data.shape}')\n",
    "    data=pandas.DataFrame({\n",
    "        feature_names[0]:submap_data[:,0],\n",
    "        feature_names[1]:submap_data[:,1],\n",
    "        feature_names[2]:submap_data[:,2],\n",
    "\n",
    "        feature_names[3]:submap_data[:,3],\n",
    "        feature_names[4]:submap_data[:,4],\n",
    "        feature_names[5]:submap_data[:,5],\n",
    "\n",
    "        feature_names[6]:submap_data[:,6],\n",
    "        feature_names[7]:submap_data[:,7],\n",
    "        feature_names[8]:submap_data[:,8],\n",
    "\n",
    "        feature_names[9]:submap_data[:,9],\n",
    "        feature_names[10]:submap_data[:,10],\n",
    "        feature_names[11]:submap_data[:,11],\n",
    "        \n",
    "        'state':submap_labels\n",
    "    })\n",
    "\n",
    "    all_data=all_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Training Data\n",
    "\n",
    "## Case 1: All good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nodes(opt_psi, opt_signal, est_psi, est_signal):\n",
    "    n_signal = len(opt_signal)\n",
    "    assert n_signal == len(opt_signal)\n",
    "#     for i in range(0, n_signal):\n",
    "        \n",
    "def create_signal_from_traj(traj):\n",
    "    traj_origin = traj[0,:]\n",
    "    pos_signal = (traj - traj_origin).squeeze()\n",
    "    x_pos = np.linalg.norm(pos_signal, ord=2, axis=1)\n",
    "    return x_pos\n",
    "\n",
    "def compute_features(coeffs_1, coeffs_2):\n",
    "    n_dim = coeffs_1.shape[0]\n",
    "    assert n_dim == coeffs_2.shape[0]\n",
    "    \n",
    "    D = compute_distances(coeffs_1, coeffs_2)\n",
    "    data = np.array([            \n",
    "        # Euclidean distance.\n",
    "        np.sum(D[0, 0:2]),\n",
    "        np.sum(D[0, 2:4]),\n",
    "        np.sum(D[0, 5:]),            \n",
    "\n",
    "        # Correlation.\n",
    "        np.sum(D[1, 0:2]),\n",
    "        np.sum(D[1, 2:4]),\n",
    "        np.sum(D[1, 5:]),\n",
    "\n",
    "        # Cityblock distance.\n",
    "        np.sum(D[2, 0:2]),\n",
    "        np.sum(D[2, 2:4]),\n",
    "        np.sum(D[2, 5:]),\n",
    "\n",
    "        # Chebyshev distance.\n",
    "        np.sum(D[3, 0:2]),\n",
    "        np.sum(D[3, 2:4]),\n",
    "        np.sum(D[3, 5:]),            \n",
    "    ])\n",
    "    \n",
    "    return np.nan_to_num(data)\n",
    "\n",
    "def compute_distances(coeffs_1, coeffs_2):\n",
    "    n_scales = coeffs_1.shape[1]\n",
    "    distances = np.zeros((9, n_scales))                \n",
    "    for j in range(0, n_scales):                              \n",
    "        distances[0, j] = scipy.spatial.distance.euclidean(coeffs_1[:,j], coeffs_2[:,j])\n",
    "        distances[1, j] = scipy.spatial.distance.correlation(coeffs_1[:,j], coeffs_2[:,j])\n",
    "        distances[2, j] = scipy.spatial.distance.cityblock(coeffs_1[:,j], coeffs_2[:,j])         \n",
    "        distances[3, j] = scipy.spatial.distance.chebyshev(coeffs_1[:,j], coeffs_2[:,j])\n",
    "\n",
    "    return distances\n",
    "    \n",
    "opt_traj = np.array([[0,0,0],[1,0,1],[1,2,1],[2,2,1],[3,3,1]])\n",
    "opt_graph = graphs.NNGraph(opt_traj, use_flann=False, k=3)\n",
    "opt_graph.compute_fourier_basis()\n",
    "opt_graph.plot()\n",
    "\n",
    "opt_eval = WaveletEvaluator()\n",
    "opt_psi = opt_eval.compute_wavelets(opt_graph)\n",
    "W_opt = opt_eval.compute_wavelets_coeffs(opt_psi, opt_signal)\n",
    "\n",
    "opt_signal = create_signal_from_traj(opt_traj)\n",
    "\n",
    "sigmas = [0.01, 0.02, 0.03, 0.05]\n",
    "for sigma in sigmas:\n",
    "    samples_1 = np.random.normal(0, sigma, G.N)\n",
    "    samples_2 = np.random.normal(0, sigma, G.N)\n",
    "    samples_3 = np.random.normal(0, sigma, G.N)\n",
    "    shifted_est_traj = opt_traj + np.array([samples_1, samples_2, samples_3]).transpose()\n",
    "    est_signal = create_signal_from_traj(shifted_est_traj)\n",
    "    \n",
    "    est_graph = graphs.NNGraph(shifted_est_traj, use_flann=False, k=3)\n",
    "    est_graph.compute_fourier_basis()\n",
    "    \n",
    "    est_eval = WaveletEvaluator()\n",
    "    est_psi = est_eval.compute_wavelets(est_graph)\n",
    "        \n",
    "    W_est = eval.compute_wavelets_coeffs(est_psi, est_signal)    \n",
    "    \n",
    "    data = compute_features(W_opt, W_est)\n",
    "    print(f'wavelet is {W_est.shape}')\n",
    "    print(f'shape of data: {data.shape}')\n",
    "    print(f'data: {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of all_data = {all_data.shape[0]}\")\n",
    "n_good = all_data[all_data['state'] == 0].shape[0]\n",
    "n_bad = all_data[all_data['state'] == 1].shape[0]\n",
    "print(f'We have {n_good} good nodes and {n_bad} bad nodes.')\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.fillna(0)\n",
    "all_data = all_data.replace(float('inf'), 1)\n",
    "\n",
    "# without JSD\n",
    "#feature_names = ['Cosine_L', 'Cosine_B', 'Cosine_H','Euclidean_L', 'Euclidean_B', 'Euclidean_H','BrayCurtis_L', 'BrayCurtis_B', 'BrayCurtis_H','Correlation_L', 'Correlation_B', 'Correlation_H', 'Canberra_L', 'Canberra_B', 'Canberra_H', 'Minkowski_L', 'Minkowski_B', 'Minkowski_H', 'Manhattan_L', 'Manhattan_B', 'Manhattan_H', 'Chebyshev_L', 'Chebyshev_B', 'Chebyshev_H']\n",
    "\n",
    "# Full\n",
    "feature_names = ['Euclidean_L', 'Euclidean_B', 'Euclidean_H','Correlation_L', 'Correlation_B', 'Correlation_H', 'Manhattan_L', 'Manhattan_B', 'Manhattan_H', 'Chebyshev_L', 'Chebyshev_B', 'Chebyshev_H']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=all_data[feature_names]\n",
    "y=all_data['state']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "dump(clf, '../config/forest.joblib') \n",
    "#clf = load('filename.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pandas.Series(clf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out one tree from the forest\n",
    "tree = clf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_names, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('../config/forest.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Cosine_L', 'Cosine_B', 'Cosine_H','Euclidean_L', 'Euclidean_B', 'Euclidean_H','BrayCurtis_L', 'BrayCurtis_B', 'BrayCurtis_H','Correlation_L', 'Correlation_B', 'Correlation_H', 'Canberra_L', 'Canberra_B', 'Canberra_H', 'JSD_L', 'JSD_B', 'JSD_H', 'Minkowski_L', 'Minkowski_B', 'Minkowski_H', 'Manhattan_L', 'Manhattan_B', 'Manhattan_H', 'Chebyshev_L', 'Chebyshev_B', 'Chebyshev_H']\n",
    "\n",
    "W_1 = eval.compute_wavelets_coeffs(psi, x_opt)\n",
    "W_2 = eval.compute_wavelets_coeffs(psi, x_loam)\n",
    "#W_2 = eval.compute_wavelets_coeffs(psi, x_rovio)\n",
    "\n",
    "n_submaps = 40\n",
    "submap_size = int(G.N/n_submaps)\n",
    "\n",
    "n_all_good = 0\n",
    "n_low_good = 0\n",
    "n_high_good = 0\n",
    "n_no_good = 0\n",
    "\n",
    "indices = []\n",
    "submap_indices = []\n",
    "state_signal = np.zeros((G.N, 1))\n",
    "\n",
    "submap_data = []\n",
    "for i in range(0, G.N):\n",
    "    n_indices_in_submap = len(submap_indices)\n",
    "\n",
    "    # If the submap reached max size, evaluate it.\n",
    "\n",
    "    if n_indices_in_submap > 0 and n_indices_in_submap % submap_size == 0:        \n",
    "        features = eval.compute_features_for_submap(W_1, W_2, submap_indices)\n",
    "        submap_data.append(features)\n",
    "\n",
    "        submap_indices = []\n",
    "\n",
    "    submap_indices.append(i)\n",
    "\n",
    "submap_data = np.array(submap_data)                \n",
    "data=pandas.DataFrame({\n",
    "    feature_names[0]:submap_data[:,0],\n",
    "    feature_names[1]:submap_data[:,1],\n",
    "    feature_names[2]:submap_data[:,2],\n",
    "\n",
    "    feature_names[3]:submap_data[:,3],\n",
    "    feature_names[4]:submap_data[:,4],\n",
    "    feature_names[5]:submap_data[:,5],\n",
    "\n",
    "    feature_names[6]:submap_data[:,6],\n",
    "    feature_names[7]:submap_data[:,7],\n",
    "    feature_names[8]:submap_data[:,8],\n",
    "\n",
    "    feature_names[9]:submap_data[:,9],\n",
    "    feature_names[10]:submap_data[:,10],\n",
    "    feature_names[11]:submap_data[:,11],\n",
    "\n",
    "    feature_names[12]:submap_data[:,12],\n",
    "    feature_names[13]:submap_data[:,13],\n",
    "    feature_names[14]:submap_data[:,14],\n",
    "\n",
    "    feature_names[15]:submap_data[:,15],\n",
    "    feature_names[16]:submap_data[:,16],\n",
    "    feature_names[17]:submap_data[:,17],\n",
    "\n",
    "    feature_names[18]:submap_data[:,18],\n",
    "    feature_names[19]:submap_data[:,19],\n",
    "    feature_names[20]:submap_data[:,20],\n",
    "\n",
    "    feature_names[21]:submap_data[:,21],\n",
    "    feature_names[22]:submap_data[:,22],\n",
    "    feature_names[23]:submap_data[:,23],\n",
    "\n",
    "    feature_names[24]:submap_data[:,24],\n",
    "    feature_names[25]:submap_data[:,25],\n",
    "    feature_names[26]:submap_data[:,26]    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "data = data.replace(float('inf'), 1)\n",
    "\n",
    "prediction = clf.predict(data)\n",
    "prediction_prob = clf.predict_proba(data)\n",
    "\n",
    "print(f\"Prediction shape {prediction.shape} and prob shape {prediction_prob.shape}\")\n",
    "len(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
